\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{array}
\usepackage{multirow, graphicx}
 \usepackage{float}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{caption}
\usepackage{subcaption}

\title{Analysis of Discriminative and Generative Graphical Models on Image Classification}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  William Watson \\
  Johns Hopkins University\\
  \texttt{billwatson@jhu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  In statistical classification, there are two main approaches to learning:
  \textit{generative} and \textit{discriminative}. This proposal will outline
  a procedure to explore the performance of corresponding probabilistic
  graphical models on a simple classification task.
\end{abstract}

\section{Introduction}
For a classification task, we are interested in the probability distribution
of a class label $y$ conditioned on the sample features $\mathbf{x}$,
written as $p(y\,|\,\mathbf{x})$. We can approach modeling this
distribution in two ways: \textit{generative} or \textit{discriminative}.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \tikz{ %
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y] (x) {$\mathbf{x}$} ; %
          \edge {y} {x} ; %
        }
        \caption*{Generative}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \tikz{ %
          \node[obs] (x) {$\mathbf{x}$} ; %
          \node[latent, below=of x] (y) {$y$} ; %
          \node[factor, below=of x] (factor-node) {} ; %
          \factoredge[-] {x} {factor-node} {y} ; %
        }
        \caption*{Discriminative}
    \end{subfigure}
\end{figure*}

\subsection{Generative Approach}
A generative model is a statistical process to describe the observed data
$\mathbf{x}$. More specifically, generative classifiers seek to
model the joint probability $p(\mathbf{x},\, y)$ from our data\cite{NgJordan}.
Predictions can be made with Bayes rule as follows:
\begin{equation}
  \hat{y} \,=\, \argmax_y \, p(\mathbf{x}, \,y) \,=\, \argmax_y \, p(y \,|\, \mathbf{x}) \, p(\mathbf{x}) \,=\, \argmax_y \, p(\mathbf{x} \,|\, y) \, p(y)
\end{equation}

\subsection{Discriminative Approach}
In contrast, a discriminative model is a statistical process to describe
relations between the observed data $\mathbf{x}$ and some latent variables
$\mathbf{h}$. For discriminative classifiers, the latent variables are class
labels $y$, and model the posterior distribution $p(y\,|\,\mathbf{x})$
directly \cite{NgJordan}. Hence, prediction becomes:
\begin{equation}
  \hat{y} \,=\, \argmax_y \, p(y \,|\, \mathbf{x})
\end{equation}

\section{Data}
I will use the Rijkmuseum Dataset \cite{Rijksmuseum} to perform type categorization.
However, to make parameter estimation and inference tractable,
the problem will be converted to a binary (paintings/photos) task. Images
will be downsampled in size and color encoding to reduce the variable and
state space. There are 3,583 paintings (\textit{schilderij}) and 2,269 photos
(\textit{foto}).

% to reduce word count and size this is a temp section, use what I wrote for actual papers

\section{Models}
In order to compare generative and discriminative
approaches to classification, I will explore their static and general versions,
as outlined in Figure \ref{fig:models}.

\subsection{Baseline}
The baseline model will consist of a \textit{Naive Bayes} classifier, a simple generative
Bayesian network that assumes all the features $x_i \in \mathbf{x}$ are
independent from one another and the sole parent is the class label $y$. This
assumption simplifies the factorization for the joint distribution.

The discriminative analog to Naive Bayes is \textit{Logistic Regression}, which is
also a naive classifier, yet models the conditional probability directly.

\subsection{General}
The general counterparts to the baseline models are the generative
\textit{General Bayesian Network} and its discriminative analog
\textit{Conditional Random Fields (CRFs)}.

General Bayesian Networks are probabilistic models that
encode the conditional dependencies of its variables via a directed acyclic
graph. Each variable is dependent only on its parents,
allowing for more complex interactions between the features.

CRFs are an undirected probabilistic graphical model whose
nodes can be partitioned into two disjoint sets: $\mathbf{x}$ and $\mathbf{y}$,
and models the conditional distribution $p(\mathbf{y}\, | \, \mathbf{x})$
directly.

\begin{figure*}[h!]
    \centering
    \caption{Generative vs. Discriminative Models}
    \label{fig:models}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: Naive Bayes}
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \edge {y} {x1, x2, x3} ; %
        }
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: General Bayesian Networks}
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1] (x2) {$x_2$};
          \node[latent, right=of x2] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4] (x5) {$x_5$} ; %
          \node[obs, right=of x5] (x6) {$x_6$} ; %

          \edge {x1} {x4};
          \edge {x4} {x5};
          \edge {x2} {x5,x6,x3};
          \edge {x3} {x6};
          \path[->] (x1) edge[bend left] node [right] {} (x3);
        }
    \end{subfigure}

    \vspace{15pt}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Logistic Regression}
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \node[factor, below=of y, xshift=-0.5cm] (factor-node-1) {} ; %
          \node[factor, below=of y] (factor-node-2) {} ; %
          \node[factor, below=of y, xshift=0.5cm] (factor-node-3) {} ; %
          \factoredge[-] {y} {factor-node-1} {x1}; %
          \factoredge[-] {y} {factor-node-2} {x2}; %
          \factoredge[-] {y} {factor-node-3} {x3}; %
        }
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Conditional Random Fields}
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1, yshift=-0.7cm] (x2) {$x_2$};
          \node[latent, right=of x2, yshift=0.7cm] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4, xshift=0.5cm] (x5) {$x_5$} ; %
          \node[obs, below=of x3] (x6) {$x_6$} ; %
          \node[factor, below=of x1] (f1) {};
          \node[factor, above=of x2, yshift=-0.15cm] (f2) {};
          \node[factor, right=of x4, xshift=.5cm] (f3) {};
          \node[factor, below=of x3, yshift=0.15cm] (f4) {};

          \factoredge[-] {x1} {f1} {x4}; %
          \factoredge[-] {x1} {f2} {x3, x2}; %
          \factoredge[-] {x4} {f3} {x2, x5}; %
          \factoredge[-] {x3} {f4} {x2, x6}; %
        }
    \end{subfigure}
\end{figure*}

% BEGIN COMMENT SECTION

% \section{Baseline Models}
% In order to facilitate comparisons with more complex models, it is prudent to
% explore their static versions, models that encode naive independence assumptions
% amongst the features $\mathbf{x}$ and class labels $y$. As done in Ng and Jordan
% (2002) \cite{NgJordan}, the basic generative-discriminative pair is Naive Bayes
% and Logistic Regression.
%
% \subsection{Generative: Naive Bayes}
% The generative Naive Bayes classifier is a simple Bayesian network: it assumes
% each feature $x_i \in \mathbf{x}$
% is independent of one another, with class label $y$ as the sole parent node for all $x_i$.
% Hence, the model factorizes as follows:
% \begin{equation}
%   \begin{split}
%     p(y\, | \, x_1, \dots, x_n) \;\propto& \;\; p(y) \, \prod_{i=1}^n p(x_i \,|\, y) \\
%   \end{split}
% \end{equation}
%
% \subsection{Discriminative: Logistic Regression}
% The discriminative analog to Naive Bayes is Logistic Regression, and can be
% written as a log-linear model:
% \begin{equation}
%   \begin{split}
%     p(y\,|\,\mathbf{x}; w) \;=&\; \frac{1}{Z} \phi \left( \mathbf{x}, \, y \right) \\
%     Z \;=&\; \sum_y \phi(\mathbf{x}, \, y) \\
%     \phi \left( \mathbf{x}, y \right) \;=&\; \exp \left( w^T f \left( \mathbf{x}, \, y \right) \right) \\
%   \end{split}
% \end{equation}
% where $w$ is a parameter vector, $f(\mathbf{x}, \, y)$ is a feature extractor,
% and $\phi(\mathbf{x}, \, y)$ is a the potential function.
%
% \section{General Models}
% The baseline models are naive classifiers, and when discussing more complex
% interactions between variables, I can use their general formulations.
% \subsection{Generative: Bayesian Network}
% The standard Bayesian network allows for feature nodes $x_i \in \mathbf{x}$ to
% not be independent, as long as there are no cyclic structures. The factorization
% is:
% \begin{equation}
%   p(y\, | \, x_1, \dots, x_n) \;\propto \;\; p(y) \, \prod_{i=1}^n p(x_i \,|\, pa(x_i) \cup y)
% \end{equation}
% where $pa(x_i)$ are the parents of node $x_i$ with respect to the current graph.
% For images, I can model the pixels such that $pa(x_i)$ is equivalent to
% the left, top, and top-left diagonal pixels in its neighborhood.
% \subsection{Discriminative: Conditional Random Fields}
% Conditional Random Fields (CRFs) are the general case for discriminative graphical models.
% CRFs model conditional distributions $p(y\,|\,\mathbf{x})$ and takes on the form:
% \begin{equation}
%   \begin{split}
%     p(y\,|\,\mathbf{x}) \;=&\; \frac{1}{Z} \, \prod_{\mathbf{c} \in \mathcal{C}} \phi_{\mathbf{c}} \left( \mathbf{x}, \, y_{\mathbf{c}} \right) \\
%     Z \;=&\; \sum_{y} \prod_{\mathbf{c} \in \mathcal{C}} \phi_{\mathbf{c}} \left( \mathbf{x}, \, y_{\mathbf{c}} \right)
%   \end{split}
% \end{equation}

% END SECTION

\section{Parameter Estimation}
Parameter estimation for generative models will use Maximum Likelihood Estimation.
Logistic Regression will optimize its parameters via the L-BFGS algorithm.
CRFs will use an approach depending on the library used (Section \ref{software}).

\section{Inference}
Inference for generative models will use belief propagation, if tractable.
Otherwise, variational or sampling methods \cite{NUTS} will be
used to approximate the conditional posterior.

CRF inference will use loopy belief propagation or sampling techniques,
since exact inference is intractable in general graphs.
However, since Logistic Regression is a tree, message passing algorithms
yield exact solutions.

\section{Software}
\label{software}
This project will use OpenCV \cite{opencv} for image processing.
Our generative models will use pgmpy\footnote{http://pgmpy.org} for learning
and inference. Logistic Regression can be learned with scitkit-learn \cite{scikit-learn}.
CRFs will either use PyStruct \cite{pystruct} or R's CRF library\footnote{https://cran.r-project.org/web/packages/CRF/index.html},
depending on which library is easier to integrate with the preferred structure of
the task.

% \section{Additional Ideas}
% If the aforementioned outline is tractable, then I could also look into
% structured learning for Bayesian networks, running the PC algorithm to find
% a good representation of the image network.

% Might have to nix formulas and shorten descriptions

% ------------------------------

\bibliographystyle{abbrv}
\bibliography{proposal}

\end{document}
