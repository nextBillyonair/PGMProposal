\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{array}
\usepackage{multirow, graphicx}
 \usepackage{float}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{caption}
\usepackage{subcaption}

\title{Analysis of Discriminative and Generative Graphical Models on Image Classification}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  William Watson \\
  Johns Hopkins University\\
  \texttt{billwatson@jhu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  In statistical classification, there are two main approaches to learning:
  \textit{generative} and \textit{discriminative}. I outline a procedure
  to explore the performance of corresponding probabilistic graphical models
  on a simple image classification task.
\end{abstract}

\section{Introduction}
For a classification task, I are interested in the probability distribution
of a class label $y$ conditioned on the sample features $\mathbf{x}$,
written as $p(y\,|\,\mathbf{x})$. However, I can approach modeling this
distribution in two ways: \textit{generative} or \textit{discriminative}.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \tikz{ %
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y] (x) {$\mathbf{x}$} ; %
          \edge {y} {x} ; %
        }
        \caption*{Generative}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \tikz{ %
          \node[obs] (x) {$\mathbf{x}$} ; %
          \node[latent, below=of x] (y) {$y$} ; %
          \node[factor, below=of x] (factor-node) {} ; %
          \factoredge[-] {x} {factor-node} {y} ; %
        }
        \caption*{Discriminative}
    \end{subfigure}
\end{figure*}

\subsection{Generative Approach}
A generative model is a statistical process to describe the observed data
$\mathbf{x}$. More specifically, generative classifiers, as mentioned by
Ng and Jordan (2002) \cite{NgJordan}, seek to
model the joint probability $p(\mathbf{x},\, y)$ of the inputs $\mathbf{x}$ and
the labels $y$. I can then make predictions with Bayes rule as
follows:
\begin{equation}
  \hat{y} \,=\, \argmax_y \, p(\mathbf{x}, \,y) \,=\, \argmax_y \, p(y \,|\, \mathbf{x}) \, p(\mathbf{x}) \,=\, \argmax_y \, p(\mathbf{x} \,|\, y) \, p(y)
\end{equation}

\subsection{Discriminative Approach}
In contrast, a discriminative model is a statistical process to describe
relations between the observed data $\mathbf{x}$ and some latent variables
$\mathbf{h}$. For discriminative classifiers, our latent variables are class
labels $y$, and I can model the posterior distribution $p(y\,|\,\mathbf{x})$
directly \cite{NgJordan}. Hence our prediction becomes:
\begin{equation}
  \hat{y} \,=\, \argmax_y \, p(y \,|\, \mathbf{x})
\end{equation}

\section{Data}
I plan to use the Rijkmuseum Dataset \cite{Rijksmuseum} to perform artist
classification. However, to make parameter estimation and inference tractable,
I will constrain the classification problem to fewer artists and downsample
the images.

% to reduce word count and size this is a temp section, use what I wrote for actual papers

\section{Models}
In order to facilitate comparisons between the generative and discriminative
approach to classification, I will explore both the static and general versions
as outlined in Figure \ref{fig:models}.

\subsection{Baseline}
Our baseline model will consist of a \textit{Naive Bayes} classifier, a simple generative
Bayesian network that assumes all the features $x_i \in \mathbf{x}$ are
independent from one another and the sole parent is the class label $y$. This
assumption simplifies the factorization for the joint distribution.

The discriminative analog to Naive Bayes is \textit{Logistic Regression}, which is
also a naive classifier, yet models the conditional probability directly.

\subsection{General}
The general counterparts to our baseline models are the generative
\textit{General Bayesian Network} and its discriminative analog
\textit{Conditional Random Fields (CRFs)}.

General Bayesian Networks are generative probabilistic models that
encode the conditional dependencies of its variables via a directed acyclic
graph. Each variable is dependent only on its parents,
allowing for more complex interactions
between our features.

CRFs are a discriminative undirected probabilistic graphical model whose
nodes can be partitioned into two disjoint sets: $\mathbf{x}$ and $\mathbf{y}$,
and models the conditional distribution $p(\mathbf{y}\, | \, \mathbf{x})$
directly.

\begin{figure*}[h!]
    \centering
    \caption{Generative vs. Discriminative Models}
    \label{fig:models}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: Naive Bayes}
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \edge {y} {x1, x2, x3} ; %
        }
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Generative: General Bayesian Networks}
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1] (x2) {$x_2$};
          \node[latent, right=of x2] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4] (x5) {$x_5$} ; %
          \node[obs, right=of x5] (x6) {$x_6$} ; %

          \edge {x1} {x4};
          \edge {x4} {x5};
          \edge {x2} {x5,x6,x3};
          \edge {x3} {x6};
          \path[->] (x1) edge[bend left] node [right] {} (x3);
        }
    \end{subfigure}

    \vspace{15pt}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Logistic Regression}
        \tikz{ %
          \centering
          \node[latent] (y) {$y$} ; %
          \node[obs, below=of y, xshift=-1cm] (x1) {$x_1$} ; %
          \node[obs, below=of y] (x2) {$x_2$} ; %
          \node[obs, below=of y, xshift=1cm] (x3) {$x_3$} ; %
          \node[factor, below=of y, xshift=-0.5cm] (factor-node-1) {} ; %
          \node[factor, below=of y] (factor-node-2) {} ; %
          \node[factor, below=of y, xshift=0.5cm] (factor-node-3) {} ; %
          \factoredge[-] {y} {factor-node-1} {x1}; %
          \factoredge[-] {y} {factor-node-2} {x2}; %
          \factoredge[-] {y} {factor-node-3} {x3}; %
        }
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \caption*{Discriminative: Conditional Random Fields}
        \tikz{ %
          \centering
          \node[latent] (x1) {$x_1$} ; %
          \node[latent, right=of x1, yshift=-0.7cm] (x2) {$x_2$};
          \node[latent, right=of x2, yshift=0.7cm] (x3) {$x_3$};
          \node[obs, below=of x1] (x4) {$x_4$} ; %
          \node[obs, right=of x4, xshift=0.5cm] (x5) {$x_5$} ; %
          \node[obs, below=of x3] (x6) {$x_6$} ; %
          \node[factor, below=of x1] (f1) {};
          \node[factor, above=of x2, yshift=-0.15cm] (f2) {};
          \node[factor, right=of x4, xshift=.5cm] (f3) {};
          \node[factor, below=of x3, yshift=0.15cm] (f4) {};

          \factoredge[-] {x1} {f1} {x4}; %
          \factoredge[-] {x1} {f2} {x3, x2}; %
          \factoredge[-] {x4} {f3} {x2, x5}; %
          \factoredge[-] {x3} {f4} {x2, x6}; %
        }
    \end{subfigure}
\end{figure*}

% BEGIN COMMENT SECTION

% \section{Baseline Models}
% In order to facilitate comparisons with more complex models, it is prudent to
% explore their static versions, models that encode naive independence assumptions
% amongst the features $\mathbf{x}$ and class labels $y$. As done in Ng and Jordan
% (2002) \cite{NgJordan}, the basic generative-discriminative pair is Naive Bayes
% and Logistic Regression.
%
% \subsection{Generative: Naive Bayes}
% The generative Naive Bayes classifier is a simple Bayesian network: it assumes
% each feature $x_i \in \mathbf{x}$
% is independent of one another, with class label $y$ as the sole parent node for all $x_i$.
% Hence, our model factorizes as follows:
% \begin{equation}
%   \begin{split}
%     p(y\, | \, x_1, \dots, x_n) \;\propto& \;\; p(y) \, \prod_{i=1}^n p(x_i \,|\, y) \\
%   \end{split}
% \end{equation}
%
% \subsection{Discriminative: Logistic Regression}
% The discriminative analog to Naive Bayes is Logistic Regression, and can be
% written as a log-linear model:
% \begin{equation}
%   \begin{split}
%     p(y\,|\,\mathbf{x}; w) \;=&\; \frac{1}{Z} \phi \left( \mathbf{x}, \, y \right) \\
%     Z \;=&\; \sum_y \phi(\mathbf{x}, \, y) \\
%     \phi \left( \mathbf{x}, y \right) \;=&\; \exp \left( w^T f \left( \mathbf{x}, \, y \right) \right) \\
%   \end{split}
% \end{equation}
% where $w$ is a parameter vector, $f(\mathbf{x}, \, y)$ is a feature extractor,
% and $\phi(\mathbf{x}, \, y)$ is a the potential function.
%
% \section{General Models}
% The baseline models are naive classifiers, and when discussing more complex
% interactions between variables, I can use their general formulations.
% \subsection{Generative: Bayesian Network}
% The standard Bayesian network allows for feature nodes $x_i \in \mathbf{x}$ to
% not be independent, as long as there are no cyclic structures. The factorization
% is:
% \begin{equation}
%   p(y\, | \, x_1, \dots, x_n) \;\propto \;\; p(y) \, \prod_{i=1}^n p(x_i \,|\, pa(x_i) \cup y)
% \end{equation}
% where $pa(x_i)$ are the parents of node $x_i$ with respect to the current graph.
% For images, I can model the pixels such that $pa(x_i)$ is equivalent to
% the left, top, and top-left diagonal pixels in its neighborhood.
% \subsection{Discriminative: Conditional Random Fields}
% Conditional Random Fields (CRFs) are the general case for discriminative graphical models.
% CRFs model conditional distributions $p(y\,|\,\mathbf{x})$ and takes on the form:
% \begin{equation}
%   \begin{split}
%     p(y\,|\,\mathbf{x}) \;=&\; \frac{1}{Z} \, \prod_{\mathbf{c} \in \mathcal{C}} \phi_{\mathbf{c}} \left( \mathbf{x}, \, y_{\mathbf{c}} \right) \\
%     Z \;=&\; \sum_{y} \prod_{\mathbf{c} \in \mathcal{C}} \phi_{\mathbf{c}} \left( \mathbf{x}, \, y_{\mathbf{c}} \right)
%   \end{split}
% \end{equation}

% END SECTION

\section{Parameter Estimation Task}
For the generative models, I will use Maximum Likelihood Estimation
to learn the parameters of the model.

Logistic Regression will use the L-BFGS optimization algorithm, while
General CRFs will use a cutting plane approach
to optimize a quadratic programming problem, as outline by
M{\"u}ller et. al (2014) \cite{pystruct} and Memisevic (2006) \cite{memisevic}.

\section{Inference Task}
For inference in predicting the correct class label $y$ with our
generative models, I will use belief
propagation, if possible. Otherwise I will use Markov Chain Monte Carlo (MCMC)
sampling methods with NUTS \cite{NUTS} to approximate our posteriors.

For our discriminative models, CRFs will use the suite of inference algorithms
provided by OpenGM \cite{opengm}, including Loopy Belief Propagation or
Gibbs sampling. It is important to note that for CRFs, just like Markov Random Fields,
the problem of exact inference can be intractable, hence the
usage of approximation algorithms. Logistic Regression models $p(y \, | \, x)$,
and hence inference can be seen as a sampling technique.

\section{Software}
This project will use OpenGM \cite{opengm}, PyStruct \cite{pystruct},
pgmpy\footnote{http://pgmpy.org}, and scitkit-learn \cite{scikit-learn} to
create, learning, and perform inference tasks with our models.
OpenCV \cite{opencv} will be used for image preprocessing and downsampling.

% \section{Additional Ideas}
% If the aforementioned outline is tractable, then I could also look into
% structured learning for Bayesian networks, running the PC algorithm to find
% a good representation of our image network.

% Might have to nix formulas and shorten descriptions

% ------------------------------

\bibliographystyle{abbrv}
\bibliography{proposal}

\end{document}
